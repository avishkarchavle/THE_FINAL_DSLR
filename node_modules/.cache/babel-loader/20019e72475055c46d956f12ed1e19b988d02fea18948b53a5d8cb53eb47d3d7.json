{"ast":null,"code":"// import React, { useRef, useState, useEffect } from 'react';\n// import * as tf from '@tensorflow/tfjs';\n// import '@tensorflow/tfjs';\n// import '@tensorflow/tfjs-node';\n\n// const SignLanguageRecognition = () => {\n//     const [i3dModel, setI3DModel] = useState(null);\n//     const [videoInput, setVideoInput] = useState(null);\n//     const [prediction, setPrediction] = useState('');\n//     const [textToSpeechInput, setTextToSpeechInput] = useState('');\n//     const [isSpeaking, setIsSpeaking] = useState(false);\n\n//     const videoRef = useRef(null);\n//     const speechSynthesisRef = useRef(window.speechSynthesis);\n\n//     useEffect(() => {\n//         // Load the I3D model\n//         async function loadModel() {\n//             const model = await tf.loadGraphModel('path/to/i3d/model.json');\n//             setI3DModel(model);\n//         }\n\n//         loadModel();\n//     }, []);\n\n//     const handleVideoChange = (event) => {\n//         const file = event.target.files[0];\n//         setVideoInput(file);\n//     };\n\n//     const handlePredict = async () => {\n//         if (i3dModel && videoInput) {\n//             const video = document.createElement('video');\n//             video.src = URL.createObjectURL(videoInput);\n//             video.crossOrigin = 'anonymous';\n//             video.load();\n\n//             const frames = await captureVideoFrames(video);\n//             const inputTensor = preprocessFrames(frames);\n\n//             const predictions = i3dModel.predict(inputTensor);\n//             const predictedClass = predictions.argMax(1).dataSync()[0];\n\n//             setPrediction(`Prediction: ${predictedClass}`);\n//         }\n//     };\n\n//     const captureVideoFrames = async (video) => {\n//         const frames = [];\n//         const canvas = document.createElement('canvas');\n//         const context = canvas.getContext('2d');\n//         const frameCount = 16;\n\n//         for (let i = 0; i < frameCount; i++) {\n//             context.drawImage(video, 0, 0, canvas.width, canvas.height);\n//             const imageData = context.getImageData(0, 0, canvas.width, canvas.height);\n//             frames.push(tf.browser.fromPixels(imageData));\n//         }\n\n//         return frames;\n//     };\n\n//     const preprocessFrames = (frames) => {\n//         const processedFrames = frames.map(frame => {\n//             return tf.image.resizeBilinear(frame, [224, 224])\n//                 .toFloat()\n//                 .div(tf.scalar(255))\n//                 .expandDims();\n//         });\n\n//         return tf.concat(processedFrames, 0);\n//     };\n\n//     const convertToSpeech = () => {\n//         if (textToSpeechInput.trim() !== '') {\n//             const utterance = new SpeechSynthesisUtterance(textToSpeechInput);\n//             speechSynthesisRef.current.speak(utterance);\n//             setIsSpeaking(true);\n//         }\n//     };\n\n//     useEffect(() => {\n//         return () => {\n//             if (isSpeaking) {\n//                 speechSynthesisRef.current.cancel();\n//                 setIsSpeaking(false);\n//             }\n//         };\n//     }, [isSpeaking]);\n\n//     return (\n//         <div className=\"container mt-5\">\n//             <h2 className=\"display-4 mb-4\">Sign Language Recognition</h2>\n//             <div className=\"mb-3\">\n//                 <label>Upload Video:</label>\n//                 <input type=\"file\" accept=\"video/*\" onChange={handleVideoChange} />\n//             </div>\n//             <div className=\"mb-3\">\n//                 <button className=\"btn btn-success\" onClick={handlePredict}>\n//                     Predict\n//                 </button>\n//             </div>\n//             <div className=\"mb-3\">\n//                 <p>{prediction}</p>\n//             </div>\n//             <div className=\"mb-3\">\n//                 <textarea\n//                     value={textToSpeechInput}\n//                     onChange={(e) => setTextToSpeechInput(e.target.value)}\n//                     placeholder=\"Enter text for Text-to-Speech\"\n//                 />\n//                 <br />\n//                 <button className=\"btn btn-primary\" onClick={convertToSpeech}>\n//                     Convert to Speech\n//                 </button>\n//             </div>\n//         </div>\n//     );\n// };\n//will be useful after model \n// export default SignLanguageRecognition;","map":{"version":3,"names":[],"sources":["C:/Users/grish/OneDrive/Desktop/k/dslrbackup-main/src/components/Code.js"],"sourcesContent":["// import React, { useRef, useState, useEffect } from 'react';\n// import * as tf from '@tensorflow/tfjs';\n// import '@tensorflow/tfjs';\n// import '@tensorflow/tfjs-node';\n\n// const SignLanguageRecognition = () => {\n//     const [i3dModel, setI3DModel] = useState(null);\n//     const [videoInput, setVideoInput] = useState(null);\n//     const [prediction, setPrediction] = useState('');\n//     const [textToSpeechInput, setTextToSpeechInput] = useState('');\n//     const [isSpeaking, setIsSpeaking] = useState(false);\n\n//     const videoRef = useRef(null);\n//     const speechSynthesisRef = useRef(window.speechSynthesis);\n\n//     useEffect(() => {\n//         // Load the I3D model\n//         async function loadModel() {\n//             const model = await tf.loadGraphModel('path/to/i3d/model.json');\n//             setI3DModel(model);\n//         }\n\n//         loadModel();\n//     }, []);\n\n//     const handleVideoChange = (event) => {\n//         const file = event.target.files[0];\n//         setVideoInput(file);\n//     };\n\n//     const handlePredict = async () => {\n//         if (i3dModel && videoInput) {\n//             const video = document.createElement('video');\n//             video.src = URL.createObjectURL(videoInput);\n//             video.crossOrigin = 'anonymous';\n//             video.load();\n\n//             const frames = await captureVideoFrames(video);\n//             const inputTensor = preprocessFrames(frames);\n\n//             const predictions = i3dModel.predict(inputTensor);\n//             const predictedClass = predictions.argMax(1).dataSync()[0];\n\n//             setPrediction(`Prediction: ${predictedClass}`);\n//         }\n//     };\n\n//     const captureVideoFrames = async (video) => {\n//         const frames = [];\n//         const canvas = document.createElement('canvas');\n//         const context = canvas.getContext('2d');\n//         const frameCount = 16;\n\n//         for (let i = 0; i < frameCount; i++) {\n//             context.drawImage(video, 0, 0, canvas.width, canvas.height);\n//             const imageData = context.getImageData(0, 0, canvas.width, canvas.height);\n//             frames.push(tf.browser.fromPixels(imageData));\n//         }\n\n//         return frames;\n//     };\n\n//     const preprocessFrames = (frames) => {\n//         const processedFrames = frames.map(frame => {\n//             return tf.image.resizeBilinear(frame, [224, 224])\n//                 .toFloat()\n//                 .div(tf.scalar(255))\n//                 .expandDims();\n//         });\n\n//         return tf.concat(processedFrames, 0);\n//     };\n\n//     const convertToSpeech = () => {\n//         if (textToSpeechInput.trim() !== '') {\n//             const utterance = new SpeechSynthesisUtterance(textToSpeechInput);\n//             speechSynthesisRef.current.speak(utterance);\n//             setIsSpeaking(true);\n//         }\n//     };\n\n//     useEffect(() => {\n//         return () => {\n//             if (isSpeaking) {\n//                 speechSynthesisRef.current.cancel();\n//                 setIsSpeaking(false);\n//             }\n//         };\n//     }, [isSpeaking]);\n\n//     return (\n//         <div className=\"container mt-5\">\n//             <h2 className=\"display-4 mb-4\">Sign Language Recognition</h2>\n//             <div className=\"mb-3\">\n//                 <label>Upload Video:</label>\n//                 <input type=\"file\" accept=\"video/*\" onChange={handleVideoChange} />\n//             </div>\n//             <div className=\"mb-3\">\n//                 <button className=\"btn btn-success\" onClick={handlePredict}>\n//                     Predict\n//                 </button>\n//             </div>\n//             <div className=\"mb-3\">\n//                 <p>{prediction}</p>\n//             </div>\n//             <div className=\"mb-3\">\n//                 <textarea\n//                     value={textToSpeechInput}\n//                     onChange={(e) => setTextToSpeechInput(e.target.value)}\n//                     placeholder=\"Enter text for Text-to-Speech\"\n//                 />\n//                 <br />\n//                 <button className=\"btn btn-primary\" onClick={convertToSpeech}>\n//                     Convert to Speech\n//                 </button>\n//             </div>\n//         </div>\n//     );\n// };\n//will be useful after model \n// export default SignLanguageRecognition;\n"],"mappings":"AAAA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA"},"metadata":{},"sourceType":"module","externalDependencies":[]}